# Local development values for floe-infrastructure
# Optimized for OrbStack Kubernetes (24GB RAM, 14 CPUs)
# Uses emptyDir instead of PVCs for ephemeral local development
# Resource strategy: 4.5GB platform services + 18.5GB available for transforms

# Platform Configuration (Two-Tier Architecture)
# This ConfigMap is mounted by Dagster pods to load platform.yaml at runtime
platformConfig:
  enabled: true
  # Uses the default minimal config from the template
  # To use custom config: --set-file platformConfig.content=./platform/local/platform.yaml
#
# Storage Strategy:
# - LocalStack is enabled for AWS-compatible S3 + STS + IAM (production-aligned)
# - MinIO is disabled (LocalStack provides S3)
# - LocalStack enables credential vending via STS (MinIO cannot do this)
#
# Network Access Strategy:
# - NodePort services for resilient external access (survives pod restarts)
# - No manual port-forwarding required
# - Access via localhost:<nodePort> on Docker Desktop
#
# Service URLs (after deployment):
#   Dagster UI:      http://localhost:30000  (requires floe-dagster chart)
#   Polaris API:     http://localhost:30181
#   LocalStack:      http://localhost:30566  (S3, STS, IAM, Secrets Manager)
#   Jaeger UI:       http://localhost:30686
#   Marquez API:     http://localhost:30500
#   Marquez Web:     http://localhost:30301

# ==============================================================================
# External Access Configuration (NodePort for local development)
# ==============================================================================
# These NodePorts provide resilient access that survives pod restarts.
# Port pattern: 30xxx where xxx matches the last 3-4 digits of the internal port.
localAccess:
  enabled: true
  nodeports:
    dagster: 30000        # Internal 3000 → External 30000 (floe-dagster chart)
    polaris: 30181        # Internal 8181 → External 30181
    marquezApi: 30500     # Internal 5000 → External 30500
    marquezWeb: 30301     # Internal 3001 → External 30301
    localstack: 30566     # Internal 4566 → External 30566 (replaces MinIO)
    jaeger: 30686         # Internal 16686 → External 30686

# PostgreSQL - minimal resources, no persistence
postgresql:
  enabled: true
  image:
    tag: latest  # Use latest tag for compatibility
  auth:
    postgresPassword: "floe-postgres"
    database: "dagster"
  primary:
    persistence:
      enabled: false  # Uses emptyDir - data is ephemeral
    # Platform profile: Continuous service, modest resources
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    # Fast health checks for local dev
    livenessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 3
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3

# MinIO - REMOVED (migrated to LocalStack)
# MinIO has been fully replaced by LocalStack for production-aligned credential vending via STS.
# The MinIO Helm chart dependency has been removed from Chart.yaml.
# LocalStack provides S3, STS, IAM, and Secrets Manager - everything MinIO provided plus more.

# LocalStack - AWS service emulator (S3, STS, IAM, Secrets Manager)
# Provides production-aligned credential vending via STS
localstack:
  enabled: true
  image:
    repository: localstack/localstack
    tag: "3.0"
    pullPolicy: IfNotPresent
  services: "s3,sts,iam,secretsmanager"
  region: "us-east-1"
  debug: "0"
  persistence:
    enabled: false  # Uses emptyDir - data is ephemeral
  # Storage profile: Heavy I/O service (S3 + STS + IAM + SecretsManager)
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  # Fast health checks for local dev
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    failureThreshold: 3
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    failureThreshold: 3
  # Initialization job
  init:
    enabled: true
    image:
      repository: amazon/aws-cli
      tag: "2.15.0"
      pullPolicy: IfNotPresent
    # Medallion Architecture: Separate buckets per layer
    buckets:
      - name: iceberg-bronze         # Bronze: Raw + lightly cleaned (7yr retention)
      - name: iceberg-silver         # Silver: Cleaned + enriched (2yr retention)
      - name: iceberg-gold           # Gold: Aggregated marts (90d retention)
      - name: dagster-compute-logs   # Dagster: Run stdout/stderr logs (30d retention)
      - name: cube-preaggs           # Cube: Pre-aggregation cache
    iamRole:
      name: "polaris-storage-role"
    secretsManager:
      enabled: true  # Store Polaris credentials in Secrets Manager
    # Ephemeral job - auto-cleanup via TTL
    ttlSecondsAfterFinished: 300  # 5 minutes
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi

# Polaris - minimal resources
polaris:
  enabled: true
  image:
    repository: apache/polaris
    tag: "latest"
    pullPolicy: IfNotPresent
  replicaCount: 1
  # Storage profile: Iceberg catalog REST API
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # Fast health checks for local dev
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 10
    failureThreshold: 3
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 5
    failureThreshold: 3

# Jaeger - all-in-one with memory storage
# NodePort access via localAccess.nodeports.jaeger (templates/jaeger-nodeport.yaml)
jaeger:
  enabled: true
  provisionDataStore:
    cassandra: false
  allInOne:
    enabled: true
    # Platform profile: Tracing (all-in-one, memory storage)
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
  storage:
    type: memory
  agent:
    enabled: false
  collector:
    enabled: false
  query:
    enabled: false

# Marquez - minimal resources
# Note: Marquez requires its own PostgreSQL database. The postgres-init job
# creates the marquez database automatically before Marquez starts.
marquez:
  enabled: true
  image:
    repository: marquezproject/marquez
    tag: "0.49.0"
    pullPolicy: IfNotPresent
  replicaCount: 1
  # Platform profile: Lineage tracking (API + Web UI)
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # Web UI for lineage visualization
  web:
    enabled: true

# Polaris initialization for demo
# Declarative layer configuration (v1.2.0) - Single source of truth
# Defines complete medallion architecture with per-layer storage locations
polarisInit:
  enabled: true
  catalogName: "demo_catalog"
  oauthClientId: "demo_client"
  oauthClientSecret: "demo_secret_k8s"
  storageRoleArn: "arn:aws:iam::000000000000:role/polaris-storage-role"

  # Layer configuration - REQUIRED (no legacy fallback)
  # Each layer defines: namespace, storage bucket, retention, properties
  # IMPORTANT: Namespace should NOT include catalog prefix (catalog is in URL path)
  layerConfig:
    demo:
      namespace: "demo"
      storage:
        bucket: "iceberg-bronze"
        endpoint: "http://floe-infra-localstack:4566"
      retention_days: 30
      properties:
        layer: "demo"
        owner: "data-engineering"
        purpose: "demo-raw-data"
    bronze:
      namespace: "bronze"
      storage:
        bucket: "iceberg-bronze"
        endpoint: "http://floe-infra-localstack:4566"
      retention_days: 30
      properties:
        layer: "bronze"
        owner: "data-engineering"
        purpose: "raw-ingestion"
    silver:
      namespace: "silver"
      storage:
        bucket: "iceberg-silver"
        endpoint: "http://floe-infra-localstack:4566"
      retention_days: 30
      properties:
        layer: "silver"
        owner: "data-engineering"
        purpose: "transformation"
    gold:
      namespace: "gold"
      storage:
        bucket: "iceberg-gold"
        endpoint: "http://floe-infra-localstack:4566"
      retention_days: 30
      properties:
        layer: "gold"
        owner: "analytics"
        purpose: "consumption"

# Seed data initialization for demo
# Creates demo.raw_* tables with synthetic e-commerce data
# Runs AFTER polaris-init (hook-weight: 10 > 5)
seedData:
  enabled: true
  image:
    repository: "ghcr.io/obsidian-owl/floe-demo"
    tag: "latest"
    pullPolicy: IfNotPresent
  # AWS credentials for S3 access (LocalStack uses any credentials)
  awsAccessKeyId: "minioadmin"
  awsSecretAccessKey: "minioadmin"
  # Data volume configuration (can be overridden)
  seed: 42
  customersCount: 1000
  productsCount: 100
  ordersCount: 5000
  orderItemsCount: 10000

# ==============================================================================
# Demo Credentials Secret (Three-Tier Architecture)
# ==============================================================================
# Creates a K8s Secret with demo credentials for local development.
# Production deployments should use External Secrets or Sealed Secrets instead.
#
# This secret provides:
#   - Polaris OAuth2 credentials (matches polarisInit above)
#   - LocalStack AWS credentials (any value works for LocalStack)
#   - PostgreSQL credentials (matches postgresql.auth above)
#
# Consumed by floe-dagster chart via secretKeyRef (not plain values).
# Covers: Three-Tier Architecture (Tier 3 - K8s Secret Management)
demoCredentials:
  enabled: true
  # Polaris OAuth2 (matches polarisInit.oauthClientId/Secret)
  polarisClientId: "demo_client"
  polarisClientSecret: "demo_secret_k8s"
  # LocalStack AWS (any credentials work for LocalStack)
  awsAccessKeyId: "minioadmin"
  awsSecretAccessKey: "minioadmin"
  # PostgreSQL (matches postgresql.auth above)
  postgresqlUsername: "postgres"
  postgresqlPassword: "floe-postgres"
  # Cube semantic layer (consumed by floe-cube chart)
  cubeApiSecret: "floe-demo-secret-change-in-production"
  cubeSqlPassword: "cube_password"

# ==============================================================================
# Resource Management (Lifecycle Automation from platform.yaml)
# ==============================================================================
# ResourceQuota and LimitRange enforce resource budgets from platform.yaml
# Budget: 24GB total = 4.5GB platform + 18.5GB transforms

# Namespace-level resource quota (prevent runaway resource usage)
resourceQuota:
  enabled: true
  limits:
    pods: 50                      # Max 50 pods in namespace
    requests_cpu: "8"              # 8 cores reserved
    requests_memory: 8Gi           # 8Gi guaranteed
    limits_cpu: "12"               # 12 cores burstable
    limits_memory: 20Gi            # 20Gi max (4.5Gi platform + 15.5Gi jobs)

# Container-level limit ranges (default resource limits for pods)
limitRange:
  enabled: true
  container:
    # Default limits (if pod doesn't specify) - aligns with "platform" profile
    default:
      cpu: "500m"
      memory: "512Mi"
    # Default requests (if pod doesn't specify)
    defaultRequest:
      cpu: "100m"
      memory: "256Mi"
    # Minimum allowed - prevent tiny pods
    min:
      cpu: "50m"
      memory: "64Mi"
    # Maximum allowed - aligned with "transform" profile (12Gi for Dagster run pods)
    max:
      cpu: "8000m"
      memory: "12Gi"
  pod:
    # Pod-level limits (sum of all containers)
    min:
      cpu: "50m"
      memory: "64Mi"
    max:
      cpu: "8000m"
      memory: "12Gi"
