# Kubernetes Local Platform Configuration
#
# Environment: k8s-local
# Purpose: Local Kubernetes deployment (Docker Desktop)
# Schema Version: 1.1.0 (Enterprise features)
#
# Two-Tier Configuration Architecture:
# - This file is injected as a ConfigMap by floe-infrastructure chart
# - Dagster pods mount the ConfigMap at /etc/floe/platform.yaml
# - PlatformResolver loads config via FLOE_PLATFORM_FILE env var
#
# Usage:
#   make deploy-local-infra   # Injects this as ConfigMap
#   make deploy-local-dagster # Mounts ConfigMap
#
# Required environment variables (injected from K8s Secrets):
#   POLARIS_CLIENT_ID      - OAuth2 client ID for Polaris
#   POLARIS_CLIENT_SECRET  - OAuth2 client secret for Polaris
#   AWS_ACCESS_KEY_ID      - LocalStack access key (any value works)
#   AWS_SECRET_ACCESS_KEY  - LocalStack secret key (any value works)
#
# Infrastructure (floe-infrastructure chart):
#   - Storage: LocalStack S3 (http://floe-infra-localstack:4566)
#   - Catalog: Polaris (http://floe-infra-polaris:8181)
#   - Observability: Jaeger + Marquez
#
# Service URLs (NodePort - resilient to pod restarts):
#   - Dagster UI:     http://localhost:30000
#   - Polaris API:    http://localhost:30181
#   - LocalStack:     http://localhost:30566  (S3, STS, IAM, Secrets Manager)
#   - Jaeger UI:      http://localhost:30686
#   - Marquez Web:    http://localhost:30301
#   - Marquez API:    http://localhost:30500
#
# Covers: 009-US2 (Same floe.yaml works across dev/staging/prod)

version: "1.2.0"

# =============================================================================
# Infrastructure Configuration (v1.1.0)
# =============================================================================
infrastructure:
  # Timezone for all pods (logs, timestamps)
  # Uses IANA timezone database: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  timezone: "Australia/Sydney"  # AEDT (UTC+11) during summer, AEST (UTC+10) during winter

  network:
    enabled: true
    dns:
      internal_domain: floe.local
      strategy: kubernetes
    # NodePort for resilient local access (survives pod restarts)
    local_access:
      enabled: true
      access_type: nodeport
      ports:
        dagster: 30000
        polaris: 30181
        localstack: 30566
        jaeger: 30686
        marquez_web: 30301
        marquez_api: 30500
  cloud:
    provider: local
    localstack:
      enabled: true  # Using LocalStack for S3 + STS + IAM
      endpoint: "http://floe-infra-localstack:4566"
      services:
        - s3
        - sts
        - iam
        - secretsmanager

  # =============================================================================
  # Lifecycle Management - Automated Cleanup & Resource Control
  # =============================================================================
  # Full automation for ephemeral resources (Dagster run pods, init jobs)
  # Designed for OrbStack local development (24GB RAM, 14 CPUs)
  lifecycle:
    cleanup:
      enabled: true
      # Ephemeral job pods auto-deleted after completion
      jobs:
        ttl_seconds: 300  # 5 minutes after completion (success or failure)
        history_limits:
          successful: 3   # Keep last 3 successful jobs for debugging
          failed: 5       # Keep last 5 failed jobs for troubleshooting
    # Namespace-level resource quotas prevent runaway resource usage
    resource_quotas:
      enabled: true
      namespace_limits:
        pods: 50                      # Max 50 pods in namespace
        requests_cpu: "8"              # 8 cores reserved
        requests_memory: 8Gi           # 8Gi guaranteed
        limits_cpu: "12"               # 12 cores burstable
        limits_memory: 20Gi            # 20Gi max (4.5Gi platform + 15.5Gi jobs)

  # =============================================================================
  # Resource Profiles - Service Classification by Lifecycle
  # =============================================================================
  # Three profiles: platform (continuous), storage (heavy I/O), transform (ephemeral)
  # Budget: 24GB total = 4.5GB platform + 1GB OrbStack + 18.5GB available for transforms
  resource_profiles:
    # Platform services (continuous) - PostgreSQL, Dagster webserver/daemon, Jaeger, Marquez
    # Target: 256-512Mi each, total ~4.5GB for all platform services
    platform:
      requests:
        cpu: "100m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"

    # Storage services (continuous, heavy I/O) - LocalStack, Polaris
    # LocalStack needs more memory for S3 + STS + IAM + SecretsManager
    storage:
      requests:
        cpu: "200m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"

    # Transform jobs (ephemeral, high memory) - Dagster run pods
    # DuckDB + PyArrow + dbt require 12Gi for data processing workloads
    # Allows 1-2 concurrent transform jobs within 24GB budget
    transform:
      requests:
        cpu: "1000m"       # 1 core guaranteed
        memory: "4Gi"      # 4Gi guaranteed (ensures scheduling)
      limits:
        cpu: "8000m"       # 8 cores burstable
        memory: "12Gi"     # 12Gi max (research-backed for PyArrow + DuckDB + dbt)
      # DuckDB configuration to prevent OOM
      # Set via environment variables in Dagster run pods
      env:
        DUCKDB_MEMORY_LIMIT: "8GB"  # 67% of 12Gi limit
        DUCKDB_THREADS: "4"         # Conservative thread count
        DUCKDB_TEMP_DIRECTORY: "/tmp/duckdb"  # Spill-to-disk location

# =============================================================================
# Security Configuration (v1.1.0)
# =============================================================================
security:
  authentication:
    enabled: false  # No auth for local development
    method: static
  authorization:
    enabled: false
  secret_backends:
    primary: kubernetes
    kubernetes:
      enabled: true
      namespace: floe
      mount_path: /var/run/secrets
  encryption:
    in_transit:
      enabled: false  # TLS disabled for local dev
    at_rest:
      enabled: false

# =============================================================================
# Governance Configuration (v1.1.0)
# =============================================================================
governance:
  classifications:
    enabled: false  # Disabled for local development
  retention:
    enabled: false
  compliance:
    enabled: false
  data_quality:
    enabled: false

# =============================================================================
# Storage Profiles - Medallion Architecture (Bronze/Silver/Gold)
# =============================================================================
# Best Practice: Separate buckets per layer for:
# - Layer-specific access controls (Bronze: read-only, Silver: transform, Gold: public)
# - Differentiated retention policies (Bronze: 7 years, Silver: 2 years, Gold: 90 days)
# - Performance isolation (Bronze: high write, Gold: high read)
# - Compliance (GDPR/CCPA data minimization)
storage:
  # Bronze Layer: Raw and lightly cleaned data
  # - Retention: Long-term (7 years for audit trail)
  # - Access: Data engineers (read-only)
  # - Performance: Optimized for bulk writes
  bronze:
    type: s3
    endpoint: "http://floe-infra-localstack:4566"
    region: us-east-1
    bucket: iceberg-bronze
    path_style_access: true
    credentials:
      mode: static
      secret_ref: aws-credentials

  # Silver Layer: Cleaned, deduplicated, enriched data
  # - Retention: Medium-term (2 years)
  # - Access: Data engineers and analysts (read-only)
  # - Performance: Balanced read/write
  silver:
    type: s3
    endpoint: "http://floe-infra-localstack:4566"
    region: us-east-1
    bucket: iceberg-silver
    path_style_access: true
    credentials:
      mode: static
      secret_ref: aws-credentials

  # Gold Layer: Aggregated marts for analytics
  # - Retention: Short-term (90 days)
  # - Access: Public via semantic layer
  # - Performance: Optimized for analytical queries
  gold:
    type: s3
    endpoint: "http://floe-infra-localstack:4566"
    region: us-east-1
    bucket: iceberg-gold
    path_style_access: true
    credentials:
      mode: static
      secret_ref: aws-credentials

  # Default alias points to bronze for backward compatibility
  default:
    type: s3
    endpoint: "http://floe-infra-localstack:4566"
    region: us-east-1
    bucket: iceberg-bronze
    path_style_access: true
    credentials:
      mode: static
      secret_ref: aws-credentials

# =============================================================================
# Declarative Layer Configuration (v1.2.0) - Single Source of Truth
# =============================================================================
# Layers define the complete medallion architecture in ONE place:
# - Eliminates configuration duplication across 16+ files
# - Per-layer namespace-to-storage binding (fixes NoSuchBucketException)
# - Platform engineers control all infrastructure in platform.yaml
# - Data engineers reference layers without seeing infrastructure details
#
# Each layer automatically gets:
# - Dedicated S3 bucket (from storage_ref)
# - Catalog namespace (from namespace field)
# - Retention policy (metadata only in v1.2.0)
# - Custom properties (tags, owner, compliance, etc.)
layers:
  bronze:
    name: bronze
    description: "Raw and lightly cleaned data for long-term audit compliance"
    storage_ref: bronze  # → storage.bronze (iceberg-bronze bucket)
    catalog_ref: default  # → catalogs.default
    namespace: demo_catalog.bronze  # Full namespace path
    retention_days: 30  # Short for local dev (7 years in production)
    properties:
      layer: bronze
      owner: data-engineering
      purpose: raw-ingestion

  silver:
    name: silver
    description: "Cleaned, deduplicated, and enriched data for analytics"
    storage_ref: silver  # → storage.silver (iceberg-silver bucket)
    catalog_ref: default
    namespace: demo_catalog.silver
    retention_days: 30  # Short for local dev (2 years in production)
    properties:
      layer: silver
      owner: data-engineering
      purpose: transformation

  gold:
    name: gold
    description: "Aggregated analytical marts for business intelligence"
    storage_ref: gold  # → storage.gold (iceberg-gold bucket)
    catalog_ref: default
    namespace: demo_catalog.gold
    retention_days: 30  # Short for local dev (90 days in production)
    properties:
      layer: gold
      owner: analytics
      purpose: consumption

catalogs:
  default:
    type: polaris
    # Polaris endpoint in K8s (floe-infrastructure chart)
    uri: "http://floe-infra-polaris:8181/api/catalog"
    warehouse: demo_catalog
    namespace: default
    credentials:
      mode: oauth2
      # Resolved from POLARIS_CLIENT_ID env var
      client_id:
        secret_ref: polaris-client-id
      client_secret:
        secret_ref: polaris-client-secret
      scope: "PRINCIPAL_ROLE:service_admin"
    # Disable vended credentials for LocalStack (STS support varies)
    access_delegation: none
    token_refresh_enabled: true

compute:
  default:
    type: duckdb
    properties:
      path: "/tmp/floe.duckdb"
      threads: 4
    credentials:
      mode: static

# =============================================================================
# Observability Configuration (Extended in v1.1.0)
# =============================================================================
observability:
  traces: true
  metrics: true
  lineage: true
  # Jaeger OTLP endpoint in K8s (floe-infrastructure chart)
  # Service: floe-infra-jaeger-collector (port 4317 for OTLP gRPC)
  otlp_endpoint: "http://floe-infra-jaeger-collector:4317"
  # Marquez OpenLineage endpoint in K8s (floe-infrastructure chart)
  # MUST include full path /api/v1/lineage per OpenLineage spec
  lineage_endpoint: "http://floe-infra-marquez:5000/api/v1/lineage"
  attributes:
    service.name: floe-k8s-local
    deployment.environment: k8s-local
  # Extended tracing configuration (v1.1.0)
  tracing:
    sampling:
      sampling_type: always_on  # Full tracing for local dev
      probability: 1.0
    exporters:
      jaeger: true
      xray: false
  # Extended metrics configuration (v1.1.0)
  metrics_config:
    scrape_interval: "15s"
    retention_days: 7
    exporters:
      prometheus: true
      cloudwatch: false
  # Extended logging configuration (v1.1.0)
  logging:
    level: INFO
    log_format: json
    backends:
      loki: false  # Not deployed in local K8s
      cloudwatch: false
  # Dagster compute logs storage (platform-managed)
  # Captures stdout/stderr from Dagster runs for debugging and audit
  compute_logs:
    enabled: true
    manager_type: s3  # Uses LocalStack S3 for local environment
    storage_ref: default  # References storage.default (iceberg-bronze bucket)
    bucket: dagster-compute-logs  # Separate bucket for compute logs
    prefix: "compute-logs/"
    retention_days: 30  # Keep logs for 30 days
    local_dir: /tmp/dagster-compute-logs  # Local buffer before S3 upload
    upload_interval: 30  # Upload every 30 seconds
