# Floe Dagster Helm Chart - Local Kubernetes Testing Values
#
# Optimized for Docker Desktop Kubernetes with floe-infrastructure services.
# Uses DuckDB (embedded) for compute via dbt-duckdb adapter.
# Connects to Polaris (Iceberg catalog) and MinIO (S3 storage).
#
# Prerequisites:
#   1. Deploy floe-infrastructure first:
#      helm install floe-infra charts/floe-infrastructure -n floe -f charts/floe-infrastructure/values-local.yaml
#   2. Wait for Polaris initialization to complete
#
# Usage:
#   helm install floe-dagster charts/floe-dagster \
#     --namespace floe \
#     --values charts/floe-dagster/values-local.yaml
#
# Covers: 007-FR-001 (Helm chart for Kubernetes deployment)
# Covers: 008-US1 (Deploy complete stack to Kubernetes)

# Dagster subchart configuration
dagster:
  enabled: true

  # Global environment variables for all Dagster components
  global:
    env:
      # Polaris (Iceberg REST catalog) connection
      - name: POLARIS_URI
        value: "http://floe-infra-polaris:8181/api/catalog"
      - name: POLARIS_CATALOG
        value: "demo_catalog"
      # MinIO (S3-compatible storage) connection
      - name: AWS_ENDPOINT_URL
        value: "http://floe-infra-minio:9000"
      - name: AWS_ACCESS_KEY_ID
        value: "minioadmin"
      - name: AWS_SECRET_ACCESS_KEY
        value: "minioadmin"
      - name: AWS_REGION
        value: "us-east-1"
      # DuckDB configuration for dbt
      - name: DBT_PROFILES_DIR
        value: "/app/demo"
      # Observability endpoints
      - name: JAEGER_ENDPOINT
        value: "http://floe-infra-jaeger-collector:14268/api/traces"
      - name: MARQUEZ_URL
        value: "http://floe-infra-marquez:5000"

  # User deployments configuration
  # Image: ghcr.io/obsidian-owl/floe-demo:latest
  dagster-user-deployments:
    enabled: true
    enableSubchart: true
    deployments:
      - name: "floe-demo"
        image:
          repository: "ghcr.io/obsidian-owl/floe-demo"
          tag: "latest"
          pullPolicy: IfNotPresent  # Cache locally to avoid repeated downloads
        # Note: Don't specify dagsterApiGrpcArgs - the Dockerfile ENTRYPOINT
        # already includes the full command: dagster api grpc -h 0.0.0.0 -p 3030 -m demo.orchestration.definitions
        port: 3030
        env:
          # Polaris (Iceberg REST catalog) connection
          - name: POLARIS_URI
            value: "http://floe-infra-polaris:8181/api/catalog"
          - name: POLARIS_WAREHOUSE
            value: "demo_catalog"
          # Polaris OAuth2 credentials (must match polarisInit values in floe-infrastructure)
          - name: POLARIS_CLIENT_ID
            value: "demo_client"
          - name: POLARIS_CLIENT_SECRET
            value: "demo_secret_k8s"
          # MinIO (S3-compatible storage) connection
          - name: AWS_ENDPOINT_URL
            value: "http://floe-infra-minio:9000"
          - name: AWS_ACCESS_KEY_ID
            value: "minioadmin"
          - name: AWS_SECRET_ACCESS_KEY
            value: "minioadmin"
          - name: AWS_REGION
            value: "us-east-1"
          # Observability endpoints
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: "http://floe-infra-jaeger-collector:4317"
          - name: OTEL_SERVICE_NAME
            value: "floe-demo"
          - name: OPENLINEAGE_URL
            value: "http://floe-infra-marquez:5000"
          - name: OPENLINEAGE_NAMESPACE
            value: "demo"

  # PostgreSQL for Dagster metadata
  # DISABLED: Use floe-infra-postgresql (shared infrastructure PostgreSQL)
  # This avoids PVC provisioning issues on Docker Desktop
  postgresql:
    enabled: false  # Don't deploy separate PostgreSQL
    # Point to infrastructure PostgreSQL
    postgresqlHost: "floe-infra-postgresql"
    postgresqlUsername: "postgres"
    postgresqlPassword: "floe-postgres"
    postgresqlDatabase: "dagster"

  # Minimal resource configuration for local testing
  dagsterWebserver:
    replicaCount: 1
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

  dagsterDaemon:
    enabled: true
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    # Fast run monitoring for local development
    # Default pollIntervalSeconds=120 causes 2+ min delays in failure detection
    runMonitoring:
      enabled: true
      startTimeoutSeconds: 60     # Faster timeout for runs failing to start (default: 300)
      pollIntervalSeconds: 10     # Check run status every 10s (default: 120)
    # Faster sensor evaluation for local dev
    sensors:
      useThreads: true
      numWorkers: 2
    schedules:
      useThreads: true
      numWorkers: 2

  # Use K8s run launcher for proper orchestration
  runLauncher:
    type: K8sRunLauncher
    config:
      k8sRunLauncher:
        envSecrets: []
        envConfigMaps: []

# Floe-specific configuration
floe:
  compiledArtifacts:
    enabled: false  # No CompiledArtifacts for basic testing

  externalSecrets:
    enabled: false

  sealedSecrets:
    enabled: false

# Image configuration (use default Dagster images for testing)
image:
  repository: docker.io/dagster/dagster-celery-k8s
  tag: ""  # Defaults to chart appVersion
  pullPolicy: IfNotPresent

# Minimal resource defaults for local testing
resources:
  webserver:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  daemon:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# No ingress for local testing - use port-forward
ingress:
  enabled: false
